{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c82c2342-920b-4de5-b426-3697eb329925",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from math import floor\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from bayes_opt import BayesianOptimization, UtilityFunction\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers import LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d63918f1-5b57-4103-a7cd-60c72dc509b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_acc = make_scorer(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13cffb3e-89c8-400f-ae77-b9f6ddbbe0a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_scorer(accuracy_score, response_method='predict')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d655427-6a26-4cd7-b740-00e56b21bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = pd.read_csv(r'C:\\Users\\jv028u\\OneDrive - Linde Group\\Desktop\\stats and ML\\Projects\\train.csv')\n",
    "\n",
    "train = trainSet.drop(columns=['Name', 'Ticket', 'Cabin'])\n",
    "train = train.dropna(axis=0)\n",
    "train = pd.get_dummies(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d6fe7-c0a4-4ed4-875c-bb92164dbced",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=['PassengerId', 'Survived'])\n",
    "y = train['Survived']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=111, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "037ee10f-f377-47b9-a5a1-abba51a4c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_cl_bo(neurons, activation, optimizer, learning_rate, batch_size, epochs):\n",
    "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']\n",
    "    optimizerD = {\n",
    "        'Adam': Adam(learning_rate=learning_rate), \n",
    "        'SGD': SGD(learning_rate=learning_rate),\n",
    "        'RMSprop': RMSprop(learning_rate=learning_rate), \n",
    "        'Adadelta': Adadelta(learning_rate=learning_rate),\n",
    "        'Adagrad': Adagrad(learning_rate=learning_rate), \n",
    "        'Adamax': Adamax(learning_rate=learning_rate),\n",
    "        'Nadam': Nadam(learning_rate=learning_rate), \n",
    "        'Ftrl': Ftrl(learning_rate=learning_rate)\n",
    "    }\n",
    "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "                   'elu', 'exponential', LeakyReLU, 'relu']\n",
    "    \n",
    "    neurons = round(neurons)\n",
    "    activation = activationL[round(activation)]\n",
    "    batch_size = round(batch_size)\n",
    "    epochs = round(epochs)\n",
    "\n",
    "    def nn_cl_fun():\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(neurons, input_dim=10, activation=activation))\n",
    "        nn.add(Dense(neurons, activation=activation))\n",
    "        nn.add(Dense(1, activation='sigmoid'))\n",
    "        nn.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        return nn\n",
    "\n",
    "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    score = cross_val_score(nn, X_train, y_train, scoring='accuracy', cv=kfold, fit_params={'callbacks': [es]}).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70c9d890-ee3c-46e4-9566-d71feadebc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | activa... | batch_... |  epochs   | learni... |  neurons  | optimizer |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.5144   \u001b[39m | \u001b[39m5.51     \u001b[39m | \u001b[39m335.3    \u001b[39m | \u001b[39m54.88    \u001b[39m | \u001b[39m0.7716   \u001b[39m | \u001b[39m36.58    \u001b[39m | \u001b[39m1.044    \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.5431   \u001b[39m | \u001b[35m0.2023   \u001b[39m | \u001b[35m536.2    \u001b[39m | \u001b[35m39.09    \u001b[39m | \u001b[35m0.3443   \u001b[39m | \u001b[35m99.16    \u001b[39m | \u001b[35m1.664    \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.6079   \u001b[39m | \u001b[35m0.7307   \u001b[39m | \u001b[35m735.7    \u001b[39m | \u001b[35m69.7     \u001b[39m | \u001b[35m0.2815   \u001b[39m | \u001b[35m51.96    \u001b[39m | \u001b[35m0.8286   \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.5431   \u001b[39m | \u001b[39m0.6656   \u001b[39m | \u001b[39m920.6    \u001b[39m | \u001b[39m83.52    \u001b[39m | \u001b[39m0.8422   \u001b[39m | \u001b[39m83.37    \u001b[39m | \u001b[39m6.937    \u001b[39m |\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.7639   \u001b[39m | \u001b[35m5.195    \u001b[39m | \u001b[35m851.0    \u001b[39m | \u001b[35m53.71    \u001b[39m | \u001b[35m0.03717  \u001b[39m | \u001b[35m50.87    \u001b[39m | \u001b[35m0.7373   \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.5719   \u001b[39m | \u001b[39m7.355    \u001b[39m | \u001b[39m758.2    \u001b[39m | \u001b[39m65.22    \u001b[39m | \u001b[39m0.2815   \u001b[39m | \u001b[39m99.86    \u001b[39m | \u001b[39m0.9663   \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.5719   \u001b[39m | \u001b[39m5.539    \u001b[39m | \u001b[39m588.0    \u001b[39m | \u001b[39m52.4     \u001b[39m | \u001b[39m0.7306   \u001b[39m | \u001b[39m39.05    \u001b[39m | \u001b[39m2.804    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.6115   \u001b[39m | \u001b[39m2.871    \u001b[39m | \u001b[39m957.8    \u001b[39m | \u001b[39m93.5     \u001b[39m | \u001b[39m0.8157   \u001b[39m | \u001b[39m13.07    \u001b[39m | \u001b[39m6.604    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.5719   \u001b[39m | \u001b[39m8.554    \u001b[39m | \u001b[39m845.3    \u001b[39m | \u001b[39m58.5     \u001b[39m | \u001b[39m0.9671   \u001b[39m | \u001b[39m47.53    \u001b[39m | \u001b[39m2.232    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.6438   \u001b[39m | \u001b[39m0.148    \u001b[39m | \u001b[39m230.5    \u001b[39m | \u001b[39m24.25    \u001b[39m | \u001b[39m0.1367   \u001b[39m | \u001b[39m13.0     \u001b[39m | \u001b[39m1.585    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.6616   \u001b[39m | \u001b[39m4.895    \u001b[39m | \u001b[39m342.9    \u001b[39m | \u001b[39m34.35    \u001b[39m | \u001b[39m0.1581   \u001b[39m | \u001b[39m71.47    \u001b[39m | \u001b[39m3.283    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.5719   \u001b[39m | \u001b[39m6.914    \u001b[39m | \u001b[39m735.1    \u001b[39m | \u001b[39m55.3     \u001b[39m | \u001b[39m0.5993   \u001b[39m | \u001b[39m51.55    \u001b[39m | \u001b[39m6.743    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.5431   \u001b[39m | \u001b[39m1.33     \u001b[39m | \u001b[39m925.5    \u001b[39m | \u001b[39m59.83    \u001b[39m | \u001b[39m0.5966   \u001b[39m | \u001b[39m71.62    \u001b[39m | \u001b[39m1.242    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.6619   \u001b[39m | \u001b[39m7.782    \u001b[39m | \u001b[39m585.7    \u001b[39m | \u001b[39m25.55    \u001b[39m | \u001b[39m0.3711   \u001b[39m | \u001b[39m42.54    \u001b[39m | \u001b[39m3.304    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.5719   \u001b[39m | \u001b[39m1.615    \u001b[39m | \u001b[39m340.2    \u001b[39m | \u001b[39m95.93    \u001b[39m | \u001b[39m0.6591   \u001b[39m | \u001b[39m22.15    \u001b[39m | \u001b[39m6.495    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.6802   \u001b[39m | \u001b[39m7.576    \u001b[39m | \u001b[39m242.2    \u001b[39m | \u001b[39m36.29    \u001b[39m | \u001b[39m0.8738   \u001b[39m | \u001b[39m70.65    \u001b[39m | \u001b[39m2.081    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.5719   \u001b[39m | \u001b[39m6.61     \u001b[39m | \u001b[39m694.7    \u001b[39m | \u001b[39m36.84    \u001b[39m | \u001b[39m0.804    \u001b[39m | \u001b[39m15.32    \u001b[39m | \u001b[39m2.158    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.5719   \u001b[39m | \u001b[39m1.866    \u001b[39m | \u001b[39m977.8    \u001b[39m | \u001b[39m92.75    \u001b[39m | \u001b[39m0.6797   \u001b[39m | \u001b[39m20.37    \u001b[39m | \u001b[39m6.706    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.5144   \u001b[39m | \u001b[39m0.8254   \u001b[39m | \u001b[39m703.8    \u001b[39m | \u001b[39m92.23    \u001b[39m | \u001b[39m0.3464   \u001b[39m | \u001b[39m68.75    \u001b[39m | \u001b[39m6.476    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.6023   \u001b[39m | \u001b[39m3.366    \u001b[39m | \u001b[39m817.1    \u001b[39m | \u001b[39m91.69    \u001b[39m | \u001b[39m0.624    \u001b[39m | \u001b[39m23.6     \u001b[39m | \u001b[39m2.624    \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.552    \u001b[39m | \u001b[39m5.723    \u001b[39m | \u001b[39m567.3    \u001b[39m | \u001b[39m62.58    \u001b[39m | \u001b[39m0.3588   \u001b[39m | \u001b[39m69.39    \u001b[39m | \u001b[39m3.336    \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.4856   \u001b[39m | \u001b[39m4.091    \u001b[39m | \u001b[39m299.8    \u001b[39m | \u001b[39m53.0     \u001b[39m | \u001b[39m0.2804   \u001b[39m | \u001b[39m41.21    \u001b[39m | \u001b[39m6.821    \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.5719   \u001b[39m | \u001b[39m1.94     \u001b[39m | \u001b[39m746.3    \u001b[39m | \u001b[39m22.54    \u001b[39m | \u001b[39m0.837    \u001b[39m | \u001b[39m73.15    \u001b[39m | \u001b[39m6.762    \u001b[39m |\n",
      "| \u001b[35m24       \u001b[39m | \u001b[35m0.7675   \u001b[39m | \u001b[35m5.326    \u001b[39m | \u001b[35m373.9    \u001b[39m | \u001b[35m77.54    \u001b[39m | \u001b[35m0.04056  \u001b[39m | \u001b[35m47.68    \u001b[39m | \u001b[35m1.969    \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.6833   \u001b[39m | \u001b[39m0.9562   \u001b[39m | \u001b[39m541.1    \u001b[39m | \u001b[39m87.25    \u001b[39m | \u001b[39m0.1193   \u001b[39m | \u001b[39m98.8     \u001b[39m | \u001b[39m1.633    \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.612    \u001b[39m | \u001b[39m1.194    \u001b[39m | \u001b[39m933.6    \u001b[39m | \u001b[39m76.02    \u001b[39m | \u001b[39m0.1994   \u001b[39m | \u001b[39m41.52    \u001b[39m | \u001b[39m0.4808   \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.5719   \u001b[39m | \u001b[39m2.159    \u001b[39m | \u001b[39m552.6    \u001b[39m | \u001b[39m28.08    \u001b[39m | \u001b[39m0.8456   \u001b[39m | \u001b[39m95.38    \u001b[39m | \u001b[39m5.225    \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.5431   \u001b[39m | \u001b[39m4.186    \u001b[39m | \u001b[39m843.1    \u001b[39m | \u001b[39m26.1     \u001b[39m | \u001b[39m0.9518   \u001b[39m | \u001b[39m51.28    \u001b[39m | \u001b[39m4.972    \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.7582   \u001b[39m | \u001b[39m5.326    \u001b[39m | \u001b[39m373.9    \u001b[39m | \u001b[39m77.54    \u001b[39m | \u001b[39m0.04056  \u001b[39m | \u001b[39m47.68    \u001b[39m | \u001b[39m1.969    \u001b[39m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params_nn ={\n",
    "    'neurons': (10, 100),\n",
    "    'activation':(0, 9),\n",
    "    'optimizer':(0,7),\n",
    "    'learning_rate':(0.01, 1),\n",
    "    'batch_size':(200, 1000),\n",
    "    'epochs':(20, 100)\n",
    "}\n",
    "# Run Bayesian Optimization\n",
    "nn_bo = BayesianOptimization(nn_cl_bo, params_nn, random_state=111)\n",
    "nn_bo.maximize(init_points=25, n_iter=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7733b72c-a8fb-440d-80f2-b7314ecc9b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'selu',\n",
       " 'batch_size': 373.87633695429145,\n",
       " 'epochs': 77.53809882694163,\n",
       " 'learning_rate': 0.04056128946327648,\n",
       " 'neurons': 47.68371572646683,\n",
       " 'optimizer': 1.968575834097605}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_nn_ = nn_bo.max['params']\n",
    "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "               'elu', 'exponential', LeakyReLU,'relu']\n",
    "best_params['activation'] = activationL[round(best_params['activation'])]\n",
    "\n",
    "optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']\n",
    "optimizers = {\n",
    "    'Adam': Adam(learning_rate=best_params['learning_rate']), \n",
    "    'SGD': SGD(learning_rate=best_params['learning_rate']),\n",
    "    'RMSprop': RMSprop(learning_rate=best_params['learning_rate']), \n",
    "    'Adadelta': Adadelta(learning_rate=best_params['learning_rate']),\n",
    "    'Adagrad': Adagrad(learning_rate=best_params['learning_rate']), \n",
    "    'Adamax': Adamax(learning_rate=best_params['learning_rate']),\n",
    "    'Nadam': Nadam(learning_rate=best_params['learning_rate']), \n",
    "    'Ftrl': Ftrl(learning_rate=best_params['learning_rate'])\n",
    "}\n",
    "best_optimizer = optimizers[optimizerL[round(best_params['optimizer'])]]\n",
    "\n",
    "# Build the model with the best hyperparameters\n",
    "def build_optimized_nn():\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(units=round(best_params['neurons']), input_dim=X_train.shape[1], activation=best_params['activation']))\n",
    "    nn.add(Dense(units=round(best_params['neurons']), activation=best_params['activation']))\n",
    "    nn.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "    nn.compile(loss='binary_crossentropy', optimizer=best_optimizer, metrics=['accuracy'])\n",
    "    return nn\n",
    "\n",
    "# Early stopping callback\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=10)\n",
    "\n",
    "# Train the model with optimized parameters\n",
    "model = KerasClassifier(build_fn=build_optimized_nn, \n",
    "                        epochs=round(best_params['epochs']), \n",
    "                        batch_size=round(best_params['batch_size']), \n",
    "                        verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=[es])\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Evaluate accuracy on the validation set\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "865fd947-ee56-43f6-961c-35e30cad128d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5152 - loss: 4.9662 - val_accuracy: 0.6033 - val_loss: 0.9178\n",
      "Epoch 2/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5187 - loss: 1.0720 - val_accuracy: 0.5719 - val_loss: 0.9530\n",
      "Epoch 3/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5801 - loss: 0.9513 - val_accuracy: 0.7304 - val_loss: 0.6878\n",
      "Epoch 4/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6259 - loss: 0.8378 - val_accuracy: 0.7240 - val_loss: 0.6105\n",
      "Epoch 5/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6366 - loss: 0.8719 - val_accuracy: 0.7230 - val_loss: 0.7088\n",
      "Epoch 6/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6669 - loss: 0.7949 - val_accuracy: 0.7443 - val_loss: 0.5881\n",
      "Epoch 7/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6671 - loss: 0.7959 - val_accuracy: 0.7593 - val_loss: 0.5340\n",
      "Epoch 8/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6687 - loss: 0.7724 - val_accuracy: 0.5720 - val_loss: 0.9668\n",
      "Epoch 9/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6636 - loss: 0.7866 - val_accuracy: 0.7538 - val_loss: 0.8099\n",
      "Epoch 10/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6817 - loss: 0.7727 - val_accuracy: 0.5703 - val_loss: 0.7171\n",
      "Epoch 11/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6677 - loss: 0.7833 - val_accuracy: 0.7530 - val_loss: 0.7107\n",
      "Epoch 12/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6734 - loss: 0.7794 - val_accuracy: 0.7541 - val_loss: 0.5580\n",
      "Epoch 13/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6704 - loss: 0.7804 - val_accuracy: 0.7127 - val_loss: 0.7767\n",
      "Epoch 14/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6665 - loss: 0.7806 - val_accuracy: 0.5719 - val_loss: 1.6891\n",
      "Epoch 15/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6673 - loss: 0.8088 - val_accuracy: 0.6741 - val_loss: 0.9051\n",
      "Epoch 16/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6703 - loss: 0.7834 - val_accuracy: 0.6401 - val_loss: 0.8489\n",
      "Epoch 17/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6670 - loss: 0.7836 - val_accuracy: 0.5752 - val_loss: 0.6947\n",
      "Epoch 18/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6642 - loss: 0.7820 - val_accuracy: 0.7253 - val_loss: 0.7980\n",
      "Epoch 19/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6783 - loss: 0.7740 - val_accuracy: 0.7413 - val_loss: 0.5689\n",
      "Epoch 20/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6619 - loss: 0.7846 - val_accuracy: 0.5733 - val_loss: 0.8482\n",
      "Epoch 21/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6685 - loss: 0.7670 - val_accuracy: 0.7541 - val_loss: 0.5854\n",
      "Epoch 22/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6749 - loss: 0.7647 - val_accuracy: 0.7583 - val_loss: 0.7277\n",
      "Epoch 23/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6780 - loss: 0.7638 - val_accuracy: 0.5688 - val_loss: 0.7294\n",
      "Epoch 24/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6677 - loss: 0.7792 - val_accuracy: 0.7266 - val_loss: 0.5920\n",
      "Epoch 25/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6754 - loss: 0.7511 - val_accuracy: 0.5766 - val_loss: 0.9136\n",
      "Epoch 26/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6643 - loss: 0.7712 - val_accuracy: 0.6106 - val_loss: 0.9411\n",
      "Epoch 27/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6828 - loss: 0.7538 - val_accuracy: 0.6748 - val_loss: 0.6857\n",
      "Epoch 28/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6784 - loss: 0.7397 - val_accuracy: 0.5722 - val_loss: 1.2636\n",
      "Epoch 29/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6900 - loss: 0.7383 - val_accuracy: 0.7618 - val_loss: 0.6519\n",
      "Epoch 30/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6833 - loss: 0.7348 - val_accuracy: 0.5683 - val_loss: 1.1045\n",
      "Epoch 31/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6720 - loss: 0.7704 - val_accuracy: 0.6230 - val_loss: 1.1617\n",
      "Epoch 32/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6804 - loss: 0.7371 - val_accuracy: 0.7614 - val_loss: 0.6529\n",
      "Epoch 33/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6839 - loss: 0.7315 - val_accuracy: 0.7516 - val_loss: 0.6190\n",
      "Epoch 34/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6734 - loss: 0.7496 - val_accuracy: 0.6663 - val_loss: 0.8262\n",
      "Epoch 35/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6746 - loss: 0.7524 - val_accuracy: 0.6738 - val_loss: 0.9317\n",
      "Epoch 36/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6822 - loss: 0.7340 - val_accuracy: 0.6492 - val_loss: 0.7813\n",
      "Epoch 37/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6793 - loss: 0.7338 - val_accuracy: 0.7132 - val_loss: 1.0169\n",
      "Epoch 38/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6806 - loss: 0.7398 - val_accuracy: 0.5924 - val_loss: 1.2040\n",
      "Epoch 39/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6724 - loss: 0.7423 - val_accuracy: 0.5568 - val_loss: 0.9761\n",
      "Epoch 40/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6817 - loss: 0.7289 - val_accuracy: 0.6774 - val_loss: 1.0481\n",
      "Epoch 41/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6818 - loss: 0.7370 - val_accuracy: 0.6400 - val_loss: 0.8196\n",
      "Epoch 42/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6844 - loss: 0.7271 - val_accuracy: 0.5719 - val_loss: 1.3483\n",
      "Epoch 43/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6802 - loss: 0.7460 - val_accuracy: 0.6630 - val_loss: 0.6135\n",
      "Epoch 44/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6806 - loss: 0.7252 - val_accuracy: 0.6354 - val_loss: 0.8276\n",
      "Epoch 45/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6721 - loss: 0.7512 - val_accuracy: 0.7404 - val_loss: 0.6271\n",
      "Epoch 46/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6792 - loss: 0.7278 - val_accuracy: 0.5719 - val_loss: 1.7212\n",
      "Epoch 47/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6889 - loss: 0.7435 - val_accuracy: 0.6489 - val_loss: 1.2258\n",
      "Epoch 48/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6746 - loss: 0.7374 - val_accuracy: 0.6515 - val_loss: 0.7608\n",
      "Epoch 49/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6806 - loss: 0.7338 - val_accuracy: 0.6435 - val_loss: 0.8403\n",
      "Epoch 50/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6782 - loss: 0.7309 - val_accuracy: 0.7638 - val_loss: 0.6500\n",
      "Epoch 51/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6738 - loss: 0.7560 - val_accuracy: 0.6653 - val_loss: 0.9460\n",
      "Epoch 52/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6779 - loss: 0.7327 - val_accuracy: 0.5621 - val_loss: 1.4055\n",
      "Epoch 53/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6689 - loss: 0.7404 - val_accuracy: 0.6161 - val_loss: 1.0620\n",
      "Epoch 54/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6750 - loss: 0.7427 - val_accuracy: 0.6755 - val_loss: 0.6467\n",
      "Epoch 55/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6750 - loss: 0.7376 - val_accuracy: 0.7086 - val_loss: 0.7064\n",
      "Epoch 56/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6831 - loss: 0.7196 - val_accuracy: 0.7252 - val_loss: 0.8599\n",
      "Epoch 57/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6861 - loss: 0.7375 - val_accuracy: 0.6704 - val_loss: 0.8373\n",
      "Epoch 58/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6762 - loss: 0.7289 - val_accuracy: 0.7524 - val_loss: 0.6296\n",
      "Epoch 59/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6890 - loss: 0.7406 - val_accuracy: 0.6007 - val_loss: 1.3293\n",
      "Epoch 60/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6842 - loss: 0.7430 - val_accuracy: 0.6673 - val_loss: 0.6806\n",
      "Epoch 61/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6844 - loss: 0.7466 - val_accuracy: 0.7644 - val_loss: 0.7631\n",
      "Epoch 62/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6835 - loss: 0.7324 - val_accuracy: 0.7453 - val_loss: 0.6228\n",
      "Epoch 63/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6875 - loss: 0.7365 - val_accuracy: 0.6799 - val_loss: 1.2604\n",
      "Epoch 64/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6816 - loss: 0.7470 - val_accuracy: 0.7579 - val_loss: 0.6506\n",
      "Epoch 65/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6938 - loss: 0.7270 - val_accuracy: 0.7634 - val_loss: 0.5974\n",
      "Epoch 66/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6931 - loss: 0.7197 - val_accuracy: 0.6275 - val_loss: 0.8177\n",
      "Epoch 67/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6867 - loss: 0.7299 - val_accuracy: 0.7387 - val_loss: 0.7012\n",
      "Epoch 68/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6863 - loss: 0.7362 - val_accuracy: 0.7601 - val_loss: 0.6870\n",
      "Epoch 69/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6868 - loss: 0.7417 - val_accuracy: 0.6682 - val_loss: 0.8317\n",
      "Epoch 70/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6878 - loss: 0.7369 - val_accuracy: 0.6539 - val_loss: 0.8877\n",
      "Epoch 71/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6773 - loss: 0.7646 - val_accuracy: 0.7446 - val_loss: 0.7372\n",
      "Epoch 72/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6927 - loss: 0.7389 - val_accuracy: 0.6538 - val_loss: 0.8191\n",
      "Epoch 73/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6880 - loss: 0.7266 - val_accuracy: 0.5846 - val_loss: 3.1923\n",
      "Epoch 74/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6904 - loss: 0.7829 - val_accuracy: 0.6428 - val_loss: 0.8285\n",
      "Epoch 75/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6895 - loss: 0.7405 - val_accuracy: 0.5398 - val_loss: 1.1286\n",
      "Epoch 76/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6851 - loss: 0.7394 - val_accuracy: 0.7564 - val_loss: 0.7222\n",
      "Epoch 77/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6939 - loss: 0.7322 - val_accuracy: 0.4470 - val_loss: 0.9915\n",
      "Epoch 78/78\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6771 - loss: 0.7551 - val_accuracy: 0.7148 - val_loss: 0.9275\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Validation Accuracy: 71.48%\n"
     ]
    }
   ],
   "source": [
    "best_params = nn_bo.max['params']\n",
    "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu', 'elu', 'exponential', LeakyReLU, 'relu']\n",
    "best_params['activation'] = activationL[round(best_params['activation'])]\n",
    "\n",
    "\n",
    "optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']\n",
    "optimizers = {\n",
    "    'Adam': Adam(learning_rate=best_params['learning_rate']), \n",
    "    'SGD': SGD(learning_rate=best_params['learning_rate']),\n",
    "    'RMSprop': RMSprop(learning_rate=best_params['learning_rate']), \n",
    "    'Adadelta': Adadelta(learning_rate=best_params['learning_rate']),\n",
    "    'Adagrad': Adagrad(learning_rate=best_params['learning_rate']), \n",
    "    'Adamax': Adamax(learning_rate=best_params['learning_rate']),\n",
    "    'Nadam': Nadam(learning_rate=best_params['learning_rate']), \n",
    "    'Ftrl': Ftrl(learning_rate=best_params['learning_rate'])\n",
    "}\n",
    "\n",
    "\n",
    "best_optimizer = optimizers[optimizerL[round(best_params['optimizer'])]]\n",
    "\n",
    "# Build the model with optimized hyperparameters\n",
    "def build_optimized_nn():\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(units=round(best_params['neurons']), input_dim=X_train.shape[1], activation=best_params['activation']))\n",
    "    nn.add(Dense(units=round(best_params['neurons']), activation=best_params['activation']))\n",
    "    nn.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "    nn.compile(loss='binary_crossentropy', optimizer=best_optimizer, metrics=['accuracy'])\n",
    "    return nn\n",
    "\n",
    "# Define the early stopping callback\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=10)\n",
    "\n",
    "# Initialize KerasClassifier with the optimized parameters\n",
    "model = KerasClassifier(build_fn=build_optimized_nn, \n",
    "                        epochs=round(best_params['epochs']), \n",
    "                        batch_size=round(best_params['batch_size']), \n",
    "                        verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=[es])\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Evaluate accuracy on the validation set\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3ed9f3-8f2b-494e-826c-f957c3bc64de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
